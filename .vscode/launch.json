{
  // Everything run (train and eval on 0.02 split): 
  // python -m src.training --output-dir models/distilbert-full-split002 --test-ratio 0.02 --epochs 4 --batch-size 64 --logging-steps 100000 && python -m src.perplexity_training --output-dir models/perplexity-logistic --batch-size 8 --max-length 512 --test-ratio 0.02  && python -m src.evaluate --checkpoint-path models/distilbert-full-split002 --perplexity-classifier-path models/perplexity-logistic --test-ratio 0.02 --no-run-prompt-baseline

  "version": "0.2.0",
  "configurations": [
    {
      "name": "Train DistilBERT (debug subset)",
      "type": "python",
      "request": "launch",
      "module": "src.training",
      "console": "integratedTerminal",
      "args": [
        "--output-dir",
        "models/distilbert-debug",
        "--data-limit",
        "500",
        "--test-ratio",
        "0.2",
        "--epochs",
        "1",
        "--batch-size",
        "8",
        "--logging-steps",
        "1"
        // python -m src.training --output-dir models/distilbert-debug --data-limit 50000 --test-ratio 0.2 --epochs 1 --batch-size 8 --logging-steps 20
      ]
      // Full run training test0.02: python -m src.training --output-dir models/distilbert-full-split002 --test-ratio 0.02 --epochs 4 --batch-size 64 --logging-steps 10000
      // Full run: python -m src.training --output-dir models/distilbert-full --test-ratio 0.2 --epochs 4 --batch-size 64 --logging-steps 10000
    },
    {
      "name": "Train Perplexity Logistic (debug subset)",
      "type": "python",
      "request": "launch",
      "module": "src.perplexity_training",
      "console": "integratedTerminal",
      "args": [
        "--output-dir",
        "models/perplexity-logistic-debug",
        "--model-name",
        "meta-llama/Llama-3.2-1B",
        "--batch-size",
        "8",
        "--max-length",
        "512",
        "--data-limit",
        "2000",
        "--test-ratio",
        "0.2"
        // Debug:  python -m src.perplexity_training --output-dir models/perplexity-logistic-debug --batch-size 8 --max-length 512 --data-limit 500 --test-ratio 0.2
        // Full0.02: python -m src.perplexity_training --output-dir models/perplexity-logistic --batch-size 8 --max-length 512 --test-ratio 0.02
      ]
    },
    {
      "name": "Evaluate checkpoint",
      "type": "python",
      "request": "launch",
      "module": "src.evaluate",
      "console": "integratedTerminal",
      "args": [
        "--checkpoint-path",
        "models/distilbert-debug",
        "--data-limit",
        "2000",
        "--test-ratio",
        "0.2"
        // python -m src.evaluate --checkpoint-path models/distilbert-debug --data-limit 50000 --test-ratio 0.2
        ] // python -m src.evaluate --checkpoint-path models/distilbert-debug --data-limit 500 --test-ratio 0.2
      },
      // Evaluate full0.02: python -m src.evaluate --checkpoint-path models/distilbert-full-split002 --perplexity-classifier-path models/perplexity-logistic --test-ratio 0.02 --no-run-prompt-baseline
    {
      "name": "SHAP explain",
      "type": "python",
      "request": "launch",
      "module": "src.explain",
      "console": "integratedTerminal",
      "args": [
        "--checkpoint-path",
        "models/distilbert-debug",
        "--data-limit",
        "2000",
        "--test-ratio",
        "0.2",
        "--num-samples",
        "8"
      ]
    }
  ]
}
