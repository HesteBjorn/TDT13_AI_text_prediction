â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Loading dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 458625/458625 [00:55<00:00, 8311.92 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9360/9360 [00:01<00:00, 8149.84 examples/s]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/erikhbj/Documents/tdt13/TDT13_AI_text_prediction/src/models/distilbert_classifier.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(

 25%|â–ˆâ–ˆâ–       | 7166/28668 [34:35<1:43:53,  3.45it/s]

  0%|          | 0/147 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147/147 [00:14<00:00, 11.75it/s][A
                                                      

                                                 
[A
 25%|â–ˆâ–ˆâ–Œ       | 7167/28668 [34:49<1:43:53,  3.45it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147/147 [00:14<00:00, 11.75it/s][A{'eval_loss': 0.05058542266488075, 'eval_accuracy': 0.9835470085470085, 'eval_f1': 0.9915984724495363, 'eval_precision': 0.9838692216087475, 'eval_recall': 0.9994501264709117, 'eval_roc_auc': 0.992980153890448, 'eval_tp': 9088, 'eval_tn': 118, 'eval_fp': 149, 'eval_fn': 5, 'eval_runtime': 14.7414, 'eval_samples_per_second': 634.946, 'eval_steps_per_second': 9.972, 'epoch': 1.0}
Eval epoch 1.00: loss 0.0506  acc 0.9835  f1 0.9916


                                                 [A
 25%|â–ˆâ–ˆâ–Œ       | 7168/28668 [34:51<22:59:14,  3.85s/it]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 14333/28668 [1:09:24<1:09:29,  3.44it/s]

 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 146/147 [00:14<00:00, 10.12it/s][A
                                                         

                                                 
[A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 14334/28668 [1:09:39<1:09:29,  3.44it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147/147 [00:14<00:00, 10.12it/s][A{'eval_loss': 0.021542390808463097, 'eval_accuracy': 0.9924145299145299, 'eval_f1': 0.9961042524005488, 'eval_precision': 0.9939772229522558, 'eval_recall': 0.9982404047069174, 'eval_roc_auc': 0.9979277799813908, 'eval_tp': 9077, 'eval_tn': 212, 'eval_fp': 55, 'eval_fn': 16, 'eval_runtime': 14.7636, 'eval_samples_per_second': 633.99, 'eval_steps_per_second': 9.957, 'epoch': 2.0}
Eval epoch 2.00: loss 0.0215  acc 0.9924  f1 0.9961


                                                 [A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 14335/28668 [1:09:40<15:17:33,  3.84s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21500/28668 [1:44:09<34:39,  3.45it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147/147 [00:14<00:00, 11.57it/s][A
                                                       

                                                 
[A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 21501/28668 [1:44:24<34:38,  3.45it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147/147 [00:14<00:00, 11.57it/s][A{'eval_loss': 0.04320572316646576, 'eval_accuracy': 0.990491452991453, 'eval_f1': 0.9951256914398379, 'eval_precision': 0.9911629936722671, 'eval_recall': 0.9991202023534587, 'eval_roc_auc': 0.9965664825928988, 'eval_tp': 9085, 'eval_tn': 186, 'eval_fp': 81, 'eval_fn': 8, 'eval_runtime': 14.7103, 'eval_samples_per_second': 636.287, 'eval_steps_per_second': 9.993, 'epoch': 3.0}
Eval epoch 3.00: loss 0.0432  acc 0.9905  f1 0.9951


                                                 [A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 21502/28668 [1:44:25<7:38:14,  3.84s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 28667/28668 [2:18:54<00:00,  3.45it/s]

  0%|          | 0/147 [00:00<?, ?it/s][A

 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 146/147 [00:14<00:00, 10.10it/s][A
                                                  
                                                 
[A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28668/28668 [2:19:09<00:00,  3.45it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147/147 [00:14<00:00, 10.10it/s][A{'eval_loss': 0.06379354745149612, 'eval_accuracy': 0.9914529914529915, 'eval_f1': 0.9956164383561644, 'eval_precision': 0.9921371628262532, 'eval_recall': 0.9991202023534587, 'eval_roc_auc': 0.9971637235046426, 'eval_tp': 9085, 'eval_tn': 195, 'eval_fp': 72, 'eval_fn': 8, 'eval_runtime': 14.6076, 'eval_samples_per_second': 640.76, 'eval_steps_per_second': 10.063, 'epoch': 4.0}
Eval epoch 4.00: loss 0.0638  acc 0.9915  f1 0.9956


                                                 [A
                                                       

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28668/28668 [2:19:10<00:00,  3.45it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28668/28668 [2:19:10<00:00,  3.43it/s]
{'train_runtime': 8350.1876, 'train_samples_per_second': 219.696, 'train_steps_per_second': 3.433, 'train_loss': 0.021621557198193544, 'epoch': 4.0}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  0%|          | 0/147 [00:00<?, ?it/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 146/147 [00:14<00:00, 10.10it/s]Eval epoch 4.00: loss 0.0215  acc 0.9924  f1 0.9961

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147/147 [00:14<00:00, 10.07it/s]
{
    'eval_loss': 0.021542390808463097,
    'eval_accuracy': 0.9924145299145299,
    'eval_f1': 0.9961042524005488,
    'eval_precision': 0.9939772229522558,
    'eval_recall': 0.9982404047069174,
    'eval_roc_auc': 0.9979277799813908,
    'eval_tp': 9077,
    'eval_tn': 212,
    'eval_fp': 55,
    'eval_fn': 16,
    'eval_runtime': 14.7129,
    'eval_samples_per_second': 636.175,
    'eval_steps_per_second': 9.991,
    'epoch': 4.0
}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Training logistic classifier â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
`torch_dtype` is deprecated! Use `dtype` instead!
Perplexity baseline inference â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 1:26:51
Saved classifier to 
models/perplexity-logistic/perplexity_classifier.pkl
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Evaluation on held-out split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Perplexity baseline inference â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:01:46
{
    'accuracy': 0.783974358974359,
    'precision': 0.9930274717612606,
    'recall': 0.7831298801275707,
    'f1': 0.8756763403836695,
    'roc_auc': 0.8663860870052322
}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Transformer checkpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Map:   0%|          | 0/458625 [00:00<?, ? examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 458625/458625 [00:54<00:00, 8340.91 examples/s]

Map:   0%|          | 0/9360 [00:00<?, ? examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9360/9360 [00:01<00:00, 8630.87 examples/s]
/home/erikhbj/Documents/tdt13/TDT13_AI_text_prediction/src/models/distilbert_classifier.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer or Trainer(

  0%|          | 0/1170 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1170/1170 [00:14<00:00, 78.08it/s]
{
    'eval_loss': 0.021542390808463097,
    'eval_model_preparation_time': 0.0007,
    'eval_accuracy': 0.9924145299145299,
    'eval_f1': 0.9961042524005488,
    'eval_precision': 0.9939772229522558,
    'eval_recall': 0.9982404047069174,
    'eval_roc_auc': 0.9979277799813908,
    'eval_tp': 9077,
    'eval_tn': 212,
    'eval_fp': 55,
    'eval_fn': 16,
    'eval_runtime': 15.0581,
    'eval_samples_per_second': 621.591,
    'eval_steps_per_second': 77.699
}
â”€â”€â”€â”€ Perplexity baseline (meta-llama/Llama-3.2-1B) â”€â”€â”€â”€â”€
`torch_dtype` is deprecated! Use `dtype` instead!
{
    'accuracy': 0.5219017094017094,
    'precision': 0.9931653139683896,
    'recall': 0.511382382052128,
    'f1': 0.6751361161524501,
    'roc_auc': 0.7314922249530548,
    'tp': 4650,
    'tn': 235,
    'fp': 32,
    'fn': 4443,
    'avg_nll': 1.9813714027404785
}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Perplexity logistic classifier â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Perplexity baseline inference â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:01:45
{
    'accuracy': 0.783974358974359,
    'precision': 0.9930274717612606,
    'recall': 0.7831298801275707,
    'f1': 0.8756763403836695,
    'roc_auc': 0.8663860870052322,
    'tp': 7121,
    'tn': 217,
    'fp': 50,
    'fn': 1972
}
